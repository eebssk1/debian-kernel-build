--- a/net/ipv4/tcp_bbr.c	2024-01-02 00:18:21.000000000 +0800
+++ b/net/ipv4/tcp_bbr.c	2024-01-04 16:49:56.190944442 +0800
@@ -82,7 +82,7 @@
 #define BW_SCALE 24
 #define BW_UNIT (1 << BW_SCALE)
 
-#define BBR_SCALE 8	/* scaling factor for fractions in BBR (e.g. gains) */
+#define BBR_SCALE 10	/* scaling factor for fractions in BBR (e.g. gains) */
 #define BBR_UNIT (1 << BBR_SCALE)
 
 /* BBR has the following modes for deciding how fast to send: */
@@ -181,14 +181,14 @@
 };
 
 /* Window length of min_rtt filter (in sec): */
-static const u32 bbr_min_rtt_win_sec = 10;
+static const u32 bbr_min_rtt_win_sec = 6;
 /* Minimum time (in ms) spent at bbr_cwnd_min_target in BBR_PROBE_RTT mode: */
-static const u32 bbr_probe_rtt_mode_ms = 200;
+static const u32 bbr_probe_rtt_mode_ms = 160;
 /* Window length of probe_rtt_min_us filter (in ms), and consequently the
  * typical interval between PROBE_RTT mode entries. The default is 5000ms.
  * Note that bbr_probe_rtt_win_ms must be <= bbr_min_rtt_win_sec * MSEC_PER_SEC
  */
-static const u32 bbr_probe_rtt_win_ms = 5000;
+static const u32 bbr_probe_rtt_win_ms = 3000;
 /* Proportion of cwnd to estimated BDP in PROBE_RTT, in units of BBR_UNIT: */
 static const u32 bbr_probe_rtt_cwnd_gain = BBR_UNIT * 1 / 2;
 
@@ -205,7 +205,7 @@
  * lower than the estimated bandwidth. This is an important aspect of the
  * design.
  */
-static const int bbr_pacing_margin_percent = 1;
+static const int bbr_pacing_margin_percent = 2;
 
 /* We use a startup_pacing_gain of 4*ln(2) because it's the smallest value
  * that will allow a smoothly increasing pacing rate that will double each RTT
@@ -214,17 +214,17 @@
  */
 static const int bbr_startup_pacing_gain = BBR_UNIT * 277 / 100 + 1;
 /* The gain for deriving startup cwnd: */
-static const int bbr_startup_cwnd_gain = BBR_UNIT * 2;
+static const int bbr_startup_cwnd_gain = BBR_UNIT * 2 + BBR_UNIT / 2;
 /* The pacing gain in BBR_DRAIN is calculated to typically drain
  * the queue created in BBR_STARTUP in a single round:
  */
 static const int bbr_drain_gain = BBR_UNIT * 1000 / 2885;
 /* The gain for deriving steady-state cwnd tolerates delayed/stretched ACKs: */
-static const int bbr_cwnd_gain  = BBR_UNIT * 2;
+static const int bbr_cwnd_gain  = BBR_UNIT * 2 + BBR_UNIT / 2;
 /* The pacing_gain values for the PROBE_BW gain cycle, to discover/share bw: */
 static const int bbr_pacing_gain[] = {
-	BBR_UNIT * 5 / 4,	/* UP: probe for more available bw */
-	BBR_UNIT * 91 / 100,	/* DOWN: drain queue and/or yield bw */
+	BBR_UNIT * 5 / 3,	/* UP: probe for more available bw */
+	BBR_UNIT * 91 / 95,	/* DOWN: drain queue and/or yield bw */
 	BBR_UNIT,		/* CRUISE: try to use pipe w/ some headroom */
 	BBR_UNIT,		/* REFILL: refill pipe to estimated 100% */
 };
@@ -243,9 +243,9 @@
 
 /* To estimate if BBR_STARTUP or BBR_BW_PROBE_UP has filled pipe... */
 /* If bw has increased significantly (1.25x), there may be more bw available: */
-static const u32 bbr_full_bw_thresh = BBR_UNIT * 5 / 4;
-/* But after 3 rounds w/o significant bw growth, estimate pipe is full: */
-static const u32 bbr_full_bw_cnt = 3;
+static const u32 bbr_full_bw_thresh = BBR_UNIT * 5 / 4 * 9 / 10;
+/* But after 4 rounds w/o significant bw growth, estimate pipe is full: */
+static const u32 bbr_full_bw_cnt = 4;
 
 /* Gain factor for adding extra_acked to target cwnd: */
 static const int bbr_extra_acked_gain = BBR_UNIT;
@@ -254,7 +254,7 @@
 /* Max allowed val for ack_epoch_acked, after which sampling epoch is reset */
 static const u32 bbr_ack_epoch_acked_reset_thresh = 1U << 20;
 /* Time period for clamping cwnd increment due to ack aggregation */
-static const u32 bbr_extra_acked_max_us = 100 * 1000;
+static const u32 bbr_extra_acked_max_us = 100 * 1050;
 
 /* Flags to control BBR ECN-related behavior... */
 
@@ -269,7 +269,7 @@
 /* On losses, scale down inflight and pacing rate by beta scaled by BBR_SCALE.
  * No loss response when 0.
  */
-static const u32 bbr_beta = BBR_UNIT * 30 / 100;
+static const u32 bbr_beta = BBR_UNIT * 30 / 110;
 
 /* Gain factor for ECN mark ratio samples, scaled by BBR_SCALE (1/16 = 6.25%) */
 static const u32 bbr_ecn_alpha_gain = BBR_UNIT * 1 / 16;
@@ -296,7 +296,7 @@
 static const u32 bbr_ecn_reprobe_gain = BBR_UNIT * 1 / 2;
 
 /* Estimate bw probing has gone too far if loss rate exceeds this level. */
-static const u32 bbr_loss_thresh = BBR_UNIT * 2 / 100;  /* 2% loss */
+static const u32 bbr_loss_thresh = BBR_UNIT * 3 / 100;  /* 3% loss */
 
 /* Slow down for a packet loss recovered by TLP? */
 static const bool bbr_loss_probe_recovery = true;
@@ -313,14 +313,14 @@
 static const u32 bbr_full_ecn_cnt = 2;
 
 /* Fraction of unutilized headroom to try to leave in path upon high loss. */
-static const u32 bbr_inflight_headroom = BBR_UNIT * 15 / 100;
+static const u32 bbr_inflight_headroom = BBR_UNIT * 16 / 100;
 
 /* How much do we increase cwnd_gain when probing for bandwidth in
  * BBR_BW_PROBE_UP? This specifies the increment in units of
  * BBR_UNIT/4. The default is 1, meaning 0.25.
  * The min value is 0 (meaning 0.0); max is 3 (meaning 0.75).
  */
-static const u32 bbr_bw_probe_cwnd_gain = 1;
+static const u32 bbr_bw_probe_cwnd_gain = 2;
 
 /* Max number of packet-timed rounds to wait before probing for bandwidth.  If
  * we want to tolerate 1% random loss per round, and not have this cut our
@@ -329,7 +329,7 @@
  * We aim to be fair with Reno/CUBIC up to a BDP of at least:
  *  BDP = 25Mbps * .030sec /(1514bytes) = 61.9 packets
  */
-static const u32 bbr_bw_probe_max_rounds = 63;
+static const u32 bbr_bw_probe_max_rounds = 45;
 
 /* Max amount of randomness to inject in round counting for Reno-coexistence.
  */
@@ -339,7 +339,7 @@
  * We aim to be fair with Reno/CUBIC up to an inter-loss time epoch of at least:
  *  BDP*RTT = 25Mbps * .030sec /(1514bytes) * 0.030sec = 1.9 secs
  */
-static const u32 bbr_bw_probe_base_us = 2 * USEC_PER_SEC;  /* 2 secs */
+static const u32 bbr_bw_probe_base_us = 1 * USEC_PER_SEC + USEC_PER_SEC / 4;  /* 1.25 secs */
 
 /* Use BBR-native probes spread over this many usec: */
 static const u32 bbr_bw_probe_rand_us = 1 * USEC_PER_SEC;  /* 1 secs */

--- a/include/net/tcp.h	2024-01-02 00:18:21.000000000 +0800
+++ b/include/net/tcp.h	2024-01-04 22:01:17.315941079 +0800
@@ -2259,7 +2259,7 @@
  * congestion over a single RTT. In order to avoid floating point operations,
  * this fraction should be mapped to (1 << TCP_PLB_SCALE) and passed in.
  */
-#define TCP_PLB_SCALE 8
+#define TCP_PLB_SCALE 10
 
 /* State for PLB (Protective Load Balancing) for a single TCP connection. */
 struct tcp_plb_state {
