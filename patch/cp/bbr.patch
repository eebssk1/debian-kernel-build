--- a/net/ipv4/tcp_bbr.c	2024-04-28 15:05:59.157384378 +0800
+++ b/net/ipv4/tcp_bbr.c	2024-04-28 15:05:49.587718184 +0800
@@ -181,16 +181,16 @@
 };
 
 /* Window length of min_rtt filter (in sec): */
-static const u32 bbr_min_rtt_win_sec = 10;
+static const u32 bbr_min_rtt_win_sec = 6;
 /* Minimum time (in ms) spent at bbr_cwnd_min_target in BBR_PROBE_RTT mode: */
-static const u32 bbr_probe_rtt_mode_ms = 200;
+static const u32 bbr_probe_rtt_mode_ms = 240;
 /* Window length of probe_rtt_min_us filter (in ms), and consequently the
  * typical interval between PROBE_RTT mode entries. The default is 5000ms.
  * Note that bbr_probe_rtt_win_ms must be <= bbr_min_rtt_win_sec * MSEC_PER_SEC
  */
-static const u32 bbr_probe_rtt_win_ms = 5000;
+static const u32 bbr_probe_rtt_win_ms = 2000;
 /* Proportion of cwnd to estimated BDP in PROBE_RTT, in units of BBR_UNIT: */
-static const u32 bbr_probe_rtt_cwnd_gain = BBR_UNIT * 1 / 2;
+static const u32 bbr_probe_rtt_cwnd_gain = BBR_UNIT * 1 / 2 + BBR_UNIT * 1 / 5;
 
 /* Use min_rtt to help adapt TSO burst size, with smaller min_rtt resulting
  * in bigger TSO bursts. We cut the RTT-based allowance in half
@@ -212,19 +212,19 @@
  * and send the same number of packets per RTT that an un-paced, slow-starting
  * Reno or CUBIC flow would:
  */
-static const int bbr_startup_pacing_gain = BBR_UNIT * 277 / 100 + 1;
+static const int bbr_startup_pacing_gain = BBR_UNIT * 295/ 100 + 1;
 /* The gain for deriving startup cwnd: */
-static const int bbr_startup_cwnd_gain = BBR_UNIT * 2;
+static const int bbr_startup_cwnd_gain = BBR_UNIT * 2 + BBR_UNIT * 6 / 4;
 /* The pacing gain in BBR_DRAIN is calculated to typically drain
  * the queue created in BBR_STARTUP in a single round:
  */
-static const int bbr_drain_gain = BBR_UNIT * 1000 / 2885;
+static const int bbr_drain_gain = BBR_UNIT * 1000 / 2863;
 /* The gain for deriving steady-state cwnd tolerates delayed/stretched ACKs: */
-static const int bbr_cwnd_gain  = BBR_UNIT * 2;
+static const int bbr_cwnd_gain  = BBR_UNIT * 2 + BBR_UNIT * 9 / 5;
 /* The pacing_gain values for the PROBE_BW gain cycle, to discover/share bw: */
 static const int bbr_pacing_gain[] = {
-	BBR_UNIT * 5 / 4,	/* UP: probe for more available bw */
-	BBR_UNIT * 91 / 100,	/* DOWN: drain queue and/or yield bw */
+	BBR_UNIT * 34 /20,	/* UP: probe for more available bw */
+	BBR_UNIT * 95 / 100,	/* DOWN: drain queue and/or yield bw */
 	BBR_UNIT,		/* CRUISE: try to use pipe w/ some headroom */
 	BBR_UNIT,		/* REFILL: refill pipe to estimated 100% */
 };
@@ -239,13 +239,13 @@
  * smooth functioning, a sliding window protocol ACKing every other packet
  * needs at least 4 packets in flight:
  */
-static const u32 bbr_cwnd_min_target = 4;
+static const u32 bbr_cwnd_min_target = 5;
 
 /* To estimate if BBR_STARTUP or BBR_BW_PROBE_UP has filled pipe... */
 /* If bw has increased significantly (1.25x), there may be more bw available: */
-static const u32 bbr_full_bw_thresh = BBR_UNIT * 5 / 4;
+static const u32 bbr_full_bw_thresh = BBR_UNIT * 60 / 50;
 /* But after 3 rounds w/o significant bw growth, estimate pipe is full: */
-static const u32 bbr_full_bw_cnt = 3;
+static const u32 bbr_full_bw_cnt = 6;
 
 /* Gain factor for adding extra_acked to target cwnd: */
 static const int bbr_extra_acked_gain = BBR_UNIT;
@@ -254,7 +254,7 @@
 /* Max allowed val for ack_epoch_acked, after which sampling epoch is reset */
 static const u32 bbr_ack_epoch_acked_reset_thresh = 1U << 20;
 /* Time period for clamping cwnd increment due to ack aggregation */
-static const u32 bbr_extra_acked_max_us = 100 * 1000;
+static const u32 bbr_extra_acked_max_us = 97 * 1000;
 
 /* Flags to control BBR ECN-related behavior... */
 
@@ -269,7 +269,7 @@
 /* On losses, scale down inflight and pacing rate by beta scaled by BBR_SCALE.
  * No loss response when 0.
  */
-static const u32 bbr_beta = BBR_UNIT * 30 / 100;
+static const u32 bbr_beta = BBR_UNIT * 10 / 100;
 
 /* Gain factor for ECN mark ratio samples, scaled by BBR_SCALE (1/16 = 6.25%) */
 static const u32 bbr_ecn_alpha_gain = BBR_UNIT * 1 / 16;
@@ -296,7 +296,7 @@
 static const u32 bbr_ecn_reprobe_gain = BBR_UNIT * 1 / 2;
 
 /* Estimate bw probing has gone too far if loss rate exceeds this level. */
-static const u32 bbr_loss_thresh = BBR_UNIT * 2 / 100;  /* 2% loss */
+static const u32 bbr_loss_thresh = BBR_UNIT * 10 / 100;  /* 2% loss */
 
 /* Slow down for a packet loss recovered by TLP? */
 static const bool bbr_loss_probe_recovery = true;
@@ -305,7 +305,7 @@
  * and loss rate is higher than bbr_loss_thresh.
  * Disabled if 0.
  */
-static const u32 bbr_full_loss_cnt = 6;
+static const u32 bbr_full_loss_cnt = 8;
 
 /* Exit STARTUP if number of round trips with ECN mark rate above ecn_thresh
  * meets this count.
@@ -313,14 +313,14 @@
 static const u32 bbr_full_ecn_cnt = 2;
 
 /* Fraction of unutilized headroom to try to leave in path upon high loss. */
-static const u32 bbr_inflight_headroom = BBR_UNIT * 15 / 100;
+static const u32 bbr_inflight_headroom = BBR_UNIT * 5 / 100;
 
 /* How much do we increase cwnd_gain when probing for bandwidth in
  * BBR_BW_PROBE_UP? This specifies the increment in units of
  * BBR_UNIT/4. The default is 1, meaning 0.25.
  * The min value is 0 (meaning 0.0); max is 3 (meaning 0.75).
  */
-static const u32 bbr_bw_probe_cwnd_gain = 1;
+static const u32 bbr_bw_probe_cwnd_gain = 3;
 
 /* Max number of packet-timed rounds to wait before probing for bandwidth.  If
  * we want to tolerate 1% random loss per round, and not have this cut our
@@ -329,7 +329,7 @@
  * We aim to be fair with Reno/CUBIC up to a BDP of at least:
  *  BDP = 25Mbps * .030sec /(1514bytes) = 61.9 packets
  */
-static const u32 bbr_bw_probe_max_rounds = 63;
+static const u32 bbr_bw_probe_max_rounds = 30;
 
 /* Max amount of randomness to inject in round counting for Reno-coexistence.
  */
@@ -339,10 +339,10 @@
  * We aim to be fair with Reno/CUBIC up to an inter-loss time epoch of at least:
  *  BDP*RTT = 25Mbps * .030sec /(1514bytes) * 0.030sec = 1.9 secs
  */
-static const u32 bbr_bw_probe_base_us = 2 * USEC_PER_SEC;  /* 2 secs */
+static const u32 bbr_bw_probe_base_us = 1 * USEC_PER_SEC + USEC_PER_SEC / 5;  /* 2 secs */
 
 /* Use BBR-native probes spread over this many usec: */
-static const u32 bbr_bw_probe_rand_us = 1 * USEC_PER_SEC;  /* 1 secs */
+static const u32 bbr_bw_probe_rand_us = 1 * USEC_PER_SEC + USEC_PER_SEC / 4;  /* 1 secs */
 
 /* Use fast path if app-limited, no loss/ECN, and target cwnd was reached? */
 static const bool bbr_fast_path = true;
